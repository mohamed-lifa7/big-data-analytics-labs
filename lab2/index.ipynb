{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197c65c5",
   "metadata": {},
   "source": [
    "# Reading big data file with two different approachs\n",
    "\n",
    "This notebook compares two methods for reading a large CSV file `(train.csv)` in Python to determine the total number of rows(fake computation). The two methods are:\n",
    "\n",
    "- Using the **chunksize parameter in Pandas**.\n",
    "- Using the **Dask** library.\n",
    "\n",
    "The goal is to measure and compare the time taken by each approach to process a file with 135,000,000 rows(like ~5.5 GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8766ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e8b77",
   "metadata": {},
   "source": [
    "## Method 1: Using Pandas with chunksize\n",
    "\n",
    "This method reads the large CSV file in smaller chunks to avoid loading the entire file into memory at once. It iterates through the file piece by piece, counting the rows in each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30b7dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RAM: 170.52 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7155/2420788959.py:16: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in chunk_iterator:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 135000000\n",
      "Time taken with Pandas (chunksize): 98.38 seconds\n",
      "Peak RAM usage during processing: 170.52 MB\n",
      "Ending RAM usage: 195.41 MB\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "file_path = './data/train.csv'\n",
    "chunk_size = 150_000\n",
    "total_rows = 0\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "start_mem = process.memory_info().rss / (1024 * 1024) \n",
    "peak_mem = start_mem\n",
    "print(f\"Starting RAM: {start_mem:.2f} MB\")\n",
    "\n",
    "\n",
    "chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "# Loop through each chunk\n",
    "for chunk in chunk_iterator:\n",
    "    total_rows += len(chunk)\n",
    "\n",
    "# Stop timing\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "end_mem = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Time taken with Pandas (chunksize): {time_taken:.2f} seconds\")\n",
    "print(f\"Peak RAM usage during processing: {peak_mem:.2f} MB\")\n",
    "print(f\"Ending RAM usage: {end_mem:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fc525",
   "metadata": {},
   "source": [
    "## Method 2: Using Dask\n",
    "\n",
    "Dask is a parallel computing library that is designed to scale natively from a single machine to a cluster. It can handle datasets that are larger than memory by breaking them into smaller, manageable pieces (similar to Pandas chunks) and processing them in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4727fa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 135000000\n",
      "Time taken with Dask: 54.76 seconds\n",
      "Peak RAM usage during computation: 195.41 MB\n",
      "Ending RAM usage: 1095.54 MB\n"
     ]
    }
   ],
   "source": [
    "file_path = './data/train.csv'\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "start_mem = process.memory_info().rss / (1024 * 1024) # Convert to MB\n",
    "peak_mem = start_mem\n",
    "\n",
    "# Missing value in dataset issues.\n",
    "# now we tell Dask to treat the 'attributed_time' column as text (object)\n",
    "ddf = dd.read_csv(\n",
    "    file_path,\n",
    "    dtype={'attributed_time': 'object'},\n",
    "    blocksize=\"8MB\"\n",
    ")\n",
    "\n",
    "total_rows = len(ddf)\n",
    "\n",
    "# Stop timing\n",
    "end_time = time.time()\n",
    "end_mem = process.memory_info().rss / (1024 * 1024)\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Time taken with Dask: {time_taken:.2f} seconds\")\n",
    "print(f\"Peak RAM usage during computation: {peak_mem:.2f} MB\")\n",
    "print(f\"Ending RAM usage: {end_mem:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd7bc9",
   "metadata": {},
   "source": [
    "## Method 3: Pandas with chunksize on Compressed File (.csv.gz)\n",
    "\n",
    "This method is identical to Method 1, but reads from the compressed file. This saves significantly on disk space, but we expect a performance penalty due to the need to decompress each chunk on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378e841b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RAM: 1110.05 MB\n",
      "Total rows: 135000000\n",
      "Time taken with Pandas (chunksize, gzip): 115.04 seconds\n",
      "Ending RAM usage: 1155.93 MB\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "zip_file_path = './data/train.csv.gz' \n",
    "chunk_size = 150_000\n",
    "total_rows = 0\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "start_mem = process.memory_info().rss / (1024 * 1024) \n",
    "print(f\"Starting RAM: {start_mem:.2f} MB\")\n",
    "\n",
    "# This creates an iterator\n",
    "with pd.read_csv(\n",
    "    zip_file_path,\n",
    "    compression='gzip',\n",
    "    dtype={'attributed_time': 'object'},\n",
    "    chunksize=chunk_size,\n",
    ") as reader:\n",
    "    \n",
    "    for chunk in reader:\n",
    "        total_rows += len(chunk)\n",
    " \n",
    "\n",
    "# Stop timing\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "end_mem = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Time taken with Pandas (chunksize, gzip): {time_taken:.2f} seconds\")\n",
    "print(f\"Ending RAM usage: {end_mem:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be6ca95",
   "metadata": {},
   "source": [
    "## Comparison and Conclusion\n",
    "\n",
    "Here's a summary of the performance for reading 135 million rows on this local machine:\n",
    "\n",
    "| Library | Time Taken (seconds) | RAM Usage (MB) |\n",
    "| :--- | :---: | :---: |\n",
    "| Pandas (chunksize) | 98.38 | 170.52 |\n",
    "| Dask | 54.76 | 1095.54 |\n",
    "| Pandas (chunksize, compressed file) | 115.04 | 1155.93 |\n",
    "\n",
    "### Analysis\n",
    "\n",
    "Here are the key observations from these results:\n",
    "\n",
    "* **Time (Performance) ‚è±Ô∏è:** **Dask is the clear winner**, finishing in just **54.76 seconds**. This is because Dask is designed for parallel computing and uses multiple CPU cores to process chunks of the file simultaneously. The standard Pandas `chunksize` method was nearly twice as slow at **98.38 seconds**, as it processes chunks sequentially. Reading the compressed file was the slowest at **115.04 seconds**, which is expected due to the extra CPU overhead of decompressing each chunk.\n",
    "\n",
    "* **Storage (RAM) üíæ:** The standard **Pandas (chunksize) method was the most memory-efficient**, showing the lowest RAM usage at **~170.52 MB**. The Dask and compressed Pandas methods show much higher RAM usage *in this test*, but this is likely due to memory being held over from previous cell runs. The key takeaway is that the `chunksize` method is designed to keep peak memory low by only holding one chunk at a time.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Based on this lab:\n",
    "\n",
    "1.  **For maximum speed**, **Dask** is the best choice, especially on a local machine with multiple CPU cores where its parallelism provides a significant advantage.\n",
    "2.  **For memory-constrained environments**, **Pandas with `chunksize`** is the most robust and memory-efficient solution.\n",
    "3.  **For saving disk space**, using a **compressed file (`.gz`)** is effective, but it comes with a performance trade-off, resulting in the slowest read times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
