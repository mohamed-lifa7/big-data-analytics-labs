{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197c65c5",
   "metadata": {},
   "source": [
    "# Reading big data file with two different approachs\n",
    "\n",
    "This notebook compares two methods for reading a large CSV file `(train.csv)` in Python to determine the total number of rows(fake computation). The two methods are:\n",
    "\n",
    "- Using the **chunksize parameter in Pandas**.\n",
    "- Using the **Dask** library.\n",
    "\n",
    "The goal is to measure and compare the time taken by each approach to process a file with 135,000,000 rows(like ~5.5 GB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e8b77",
   "metadata": {},
   "source": [
    "## Method 1: Using Pandas with chunksize\n",
    "\n",
    "This method reads the large CSV file in smaller chunks to avoid loading the entire file into memory at once. It iterates through the file piece by piece, counting the rows in each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30b7dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 135000000\n",
      "Time taken with Pandas (chunksize): 111.81 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "file_path = './data/train.csv'\n",
    "chunk_size = 1_000_000\n",
    "total_rows = 0\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "# Loop through each chunk\n",
    "for chunk in chunk_iterator:\n",
    "    total_rows += len(chunk)\n",
    "\n",
    "# Stop timing\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Time taken with Pandas (chunksize): {time_taken:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fc525",
   "metadata": {},
   "source": [
    "## Method 2: Using Dask\n",
    "\n",
    "Dask is a parallel computing library that is designed to scale natively from a single machine to a cluster. It can handle datasets that are larger than memory by breaking them into smaller, manageable pieces (similar to Pandas chunks) and processing them in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4727fa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 135000000\n",
      "Time taken with Dask: 64.61 seconds\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import time\n",
    "\n",
    "file_path = './data/train.csv'\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Missing value in dataset issues.\n",
    "# now we tell Dask to treat the 'attributed_time' column as text (object)\n",
    "ddf = dd.read_csv(\n",
    "    file_path,\n",
    "    dtype={'attributed_time': 'object'}\n",
    ")\n",
    "\n",
    "total_rows = len(ddf)\n",
    "\n",
    "# Stop timing\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Time taken with Dask: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be6ca95",
   "metadata": {},
   "source": [
    "Comparison and Conclusion\n",
    "\n",
    "Here's a summary of the performance for reading 135 million rows:\n",
    "\n",
    "| Library           | Time Taken (seconds) \n",
    "|-------------------|----------------------\n",
    "| Pandas(chunksize) | 111.81               \n",
    "| Dask              | 64.61                \n",
    "\n",
    "For this task, **Dask** was significantly faster, completing the row count approximately 1.7 times quicker than the Pandas chunksize method. Dask's ability to parallelize operations allows it to process the data more efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
