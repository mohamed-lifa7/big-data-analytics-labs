{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab2: How to work with big data files (5GB+)\n",
        "\n",
        "This notebook implements and compares three different methods for reading a very large CSV file (`train.csv` from the TalkingData AdTracking competition) to perform a simple computation (counting the total number of rows).\n",
        "\n",
        "The three methods are:\n",
        "\n",
        "1.  **Pandas with `chunksize`**: Reading the file in small pieces to keep memory usage low.\n",
        "2.  **Dask**: Using a parallel computing library designed for large datasets.\n",
        "3.  **Pandas with `chunksize` on a Compressed File**: Testing if reading a `.gz` file saves disk space at the cost of time.\n",
        "\n",
        "We will compare them based on the lab requirements: **Time** (wall clock time) and **Storage** (both Peak RAM usage and Disk Space)."
      ],
      "metadata": {
        "id": "OWm28hBBmEP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Setup and Imports\n",
        "\n",
        "First, let's install the required `kaggle` library and import all the packages we'll need for this analysis."
      ],
      "metadata": {
        "id": "II_1HAp6mH8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "\n",
        "import dask.dataframe as dd\n",
        "import pandas as pd\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import sys\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "print(\"All libraries imported.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdj2OaCamNSs",
        "outputId": "5cf42241-4f27-444b-cda9-502f7880a9f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "All libraries imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Disk Storage Comparison\n",
        "\n",
        "Before we test the read times, let's create the compressed file for Method 3 and compare the **disk storage** sizes, as required by the lab."
      ],
      "metadata": {
        "id": "JxK90s27zBw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "import gzip\n",
        "\n",
        "# --- STEP 1: Upload your kaggle.json file ---\n",
        "# Get it from https://www.kaggle.com/account -> \"Create New API Token\"\n",
        "print(\"📂 Please upload your kaggle.json file\")\n",
        "uploaded = files.upload()  # Choose kaggle.json when prompted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "juqz_JKg5NSL",
        "outputId": "56d8e72f-ddf9-4ce9-eedc-2a2c89c0e38a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Please upload your kaggle.json file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d2243bcc-8440-417a-867c-e5ec3ae2484e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d2243bcc-8440-417a-867c-e5ec3ae2484e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 2: Configure Kaggle credentials ---\n",
        "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "\n",
        "# Move the uploaded file to the .kaggle directory\n",
        "# This assumes you just uploaded it in the cell above\n",
        "for fn in uploaded.keys():\n",
        "    shutil.move(fn, os.path.expanduser(\"~/.kaggle/kaggle.json\"))\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle config view  # optional: verify credentials"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZpvmPy25RlY",
        "outputId": "9fde36ba-30ac-41ce-d261-4ef6f23cb965"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration values from /root/.kaggle\n",
            "- username: mohamedlifa\n",
            "- path: None\n",
            "- proxy: None\n",
            "- competition: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 3: Set up paths ---\n",
        "data_dir = './data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "competition_name = 'talkingdata-adtracking-fraud-detection'\n",
        "train_file = os.path.join(data_dir, 'train.csv')\n",
        "compressed_file = os.path.join(data_dir, 'train.csv.gz')\n",
        "zip_path = os.path.join(data_dir, f\"{competition_name}.zip\")\n",
        "\n",
        "print(f\"Data directory set to: {data_dir}\")\n",
        "print(f\"Train file path set to: {train_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJe2Epke5U6v",
        "outputId": "091a071e-6ee0-4073-c90d-054ab246ef1d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory set to: ./data\n",
            "Train file path set to: ./data/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 4: Download competition files ---\n",
        "print(\"⬇️  Downloading competition dataset...\")\n",
        "!kaggle competitions download -c {competition_name} -p {data_dir}\n",
        "print(\"Download complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqNYYRE75XJu",
        "outputId": "9e567d53-8ad4-4d47-8258-a3f9b83b8f9a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading competition dataset...\n",
            "talkingdata-adtracking-fraud-detection.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 5: Unzip only train.csv (or fallback to train_sample.csv) ---\n",
        "print(\"\\n📦 Extracting train.csv from the zip...\")\n",
        "# This command tries to unzip train.csv. If it fails, it tries to unzip train_sample.csv\n",
        "!unzip -j -o {zip_path} train.csv -d {data_dir} || unzip -j -o {zip_path} train_sample.csv -d {data_dir}\n",
        "\n",
        "print(\"Extraction attempt complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAgHEI5a5akK",
        "outputId": "b9249ecd-4c70-4431-b21a-56a55e6ab03b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📦 Extracting train.csv from the zip...\n",
            "Archive:  ./data/talkingdata-adtracking-fraud-detection.zip\n",
            "  inflating: ./data/train.csv        \n",
            "Extraction attempt complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 6: Create compressed version for big-data lab ---\n",
        "target = train_file if os.path.exists(train_file) else os.path.join(data_dir, \"train_sample.csv\")\n",
        "\n",
        "if os.path.exists(target):\n",
        "    print(f\"\\n🗜️  Creating compressed file for: {target}\")\n",
        "    print(\"...this may take 10+ minutes, please wait...\")\n",
        "\n",
        "    with open(target, 'rb') as f_in:\n",
        "        with gzip.open(compressed_file, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "    print(\"✅ Setup complete! Compressed file saved at:\", compressed_file)\n",
        "else:\n",
        "    print(\"❌ train.csv not found in the zip. Please check dataset contents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OdRt5Ia5hD0",
        "outputId": "4a9fc711-7426-4254-d808-94d49b8e102b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🗜️  Creating compressed file for: ./data/train.csv\n",
            "...this may take 10+ minutes, please wait...\n",
            "✅ Setup complete! Compressed file saved at: ./data/train.csv.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Method 1: Using Pandas with `chunksize`\n",
        "\n",
        "This method reads the uncompressed `train.csv` file in small chunks (10,000 rows at a time) to avoid loading the entire 5.5GB+ file into memory."
      ],
      "metadata": {
        "id": "p7syYUqNmlnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process = psutil.Process(os.getpid())\n",
        "chunk_size = 150_000\n",
        "total_rows = 0\n",
        "\n",
        "print(\"Starting Method 1: Pandas (chunksize)\")\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "start_mem = process.memory_info().rss / (1024 * 1024)\n",
        "peak_mem = start_mem # Initialize peak memory\n",
        "print(f\"Starting RAM: {start_mem:.2f} MB\")\n",
        "\n",
        "try:\n",
        "    chunk_iterator = pd.read_csv(train_file, chunksize=chunk_size)\n",
        "\n",
        "    # Loop through each chunk\n",
        "    for chunk in chunk_iterator:\n",
        "        total_rows += len(chunk)\n",
        "\n",
        "        # --- FIX: Update peak memory inside the loop ---\n",
        "        current_mem = process.memory_info().rss / (1024 * 1024)\n",
        "        if current_mem > peak_mem:\n",
        "            peak_mem = current_mem\n",
        "\n",
        "    # Stop timing\n",
        "    end_time = time.time()\n",
        "    time_taken = end_time - start_time\n",
        "    end_mem = process.memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    print(\"\\n--- Results for Method 1 ---\")\n",
        "    print(f\"Total rows: {total_rows}\")\n",
        "    print(f\"Time taken: {time_taken:.2f} seconds\")\n",
        "    print(f\"Peak RAM usage: {peak_mem:.2f} MB\")\n",
        "    print(f\"Ending RAM usage: {end_mem:.2f} MB\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File not found at {train_file}. Please check the download step.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0_I1iRdmm75",
        "outputId": "77eda9cc-77ab-4ca7-b4ef-2177c69a20df"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Method 1: Pandas (chunksize)\n",
            "Starting RAM: 611.36 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2963938989.py:17: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk in chunk_iterator:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Results for Method 1 ---\n",
            "Total rows: 184903890\n",
            "Time taken: 140.00 seconds\n",
            "Peak RAM usage: 650.68 MB\n",
            "Ending RAM usage: 649.08 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Method 2: Using Dask\n",
        "\n",
        "Dask is a parallel computing library. It will also read the file in blocks, but it can process multiple blocks in parallel using multiple CPU cores, which should be faster. Dask is \"lazy,\" so the work only happens when we call `.compute()` or, in this case, `len()`."
      ],
      "metadata": {
        "id": "iycsAIm2mos1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process = psutil.Process(os.getpid())\n",
        "\n",
        "print(\"Starting Method 2: Dask\")\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "start_mem = process.memory_info().rss / (1024 * 1024) # Convert to MB\n",
        "print(f\"Starting RAM: {start_mem:.2f} MB\")\n",
        "\n",
        "try:\n",
        "    # Dask lazily creates a dataframe object\n",
        "    # We must specify dtype for 'attributed_time' due to mixed types in the file\n",
        "    ddf = dd.read_csv(\n",
        "        train_file,\n",
        "        dtype={'attributed_time': 'object'},\n",
        "        blocksize=\"8MB\"\n",
        "    )\n",
        "\n",
        "    ram_before_compute = process.memory_info().rss / (1024 * 1024)\n",
        "    print(f\"RAM after Dask setup (before compute): {ram_before_compute:.2f} MB\")\n",
        "\n",
        "    # This triggers the actual computation\n",
        "    total_rows = len(ddf)\n",
        "\n",
        "    # Stop timing\n",
        "    end_time = time.time()\n",
        "    time_taken = end_time - start_time\n",
        "    ram_after_compute = process.memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    print(\"\\n--- Results for Method 2 ---\")\n",
        "    print(f\"Total rows: {total_rows}\")\n",
        "    print(f\"Time taken: {time_taken:.2f} seconds\")\n",
        "    print(f\"Peak RAM usage (after compute): {ram_after_compute:.2f} MB\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File not found at {train_file}. Please check the download step.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B49FS8FlmvaT",
        "outputId": "65b7fc5f-d403-4be0-d6cd-291e2dcb582e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Method 2: Dask\n",
            "Starting RAM: 649.08 MB\n",
            "RAM after Dask setup (before compute): 656.10 MB\n",
            "\n",
            "--- Results for Method 2 ---\n",
            "Total rows: 184903890\n",
            "Time taken: 154.19 seconds\n",
            "Peak RAM usage (after compute): 663.97 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Method 3: Pandas with `chunksize` (Compressed File)\n",
        "\n",
        "This method is identical to Method 1, but it reads from the `train.csv.gz` file. We expect this to be slower because of the overhead of decompressing each chunk on the fly, but it has the advantage of using much less disk space."
      ],
      "metadata": {
        "id": "lNNv9uH4myaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process = psutil.Process(os.getpid())\n",
        "chunk_size = 150_000\n",
        "total_rows = 0\n",
        "\n",
        "print(\"Starting Method 3: Pandas (chunksize, gzip)\")\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "start_mem = process.memory_info().rss / (1024 * 1024)\n",
        "peak_mem = start_mem # Initialize\n",
        "print(f\"Starting RAM: {start_mem:.2f} MB\")\n",
        "\n",
        "try:\n",
        "    # This creates an iterator\n",
        "    with pd.read_csv(\n",
        "        compressed_file,\n",
        "        compression='gzip',\n",
        "        dtype={'attributed_time': 'object'},\n",
        "        chunksize=chunk_size,\n",
        "        on_bad_lines='skip' # Add this line to skip bad lines\n",
        "    ) as reader:\n",
        "\n",
        "        for chunk in reader:\n",
        "            total_rows += len(chunk)\n",
        "\n",
        "            # --- FIX: Update peak memory inside the loop ---\n",
        "            current_mem = process.memory_info().rss / (1024 * 1024)\n",
        "            if current_mem > peak_mem:\n",
        "                peak_mem = current_mem\n",
        "\n",
        "\n",
        "    # Stop timing\n",
        "    end_time = time.time()\n",
        "    time_taken = end_time - start_time\n",
        "    end_mem = process.memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    print(\"\\n--- Results for Method 3 ---\")\n",
        "    print(f\"Total rows: {total_rows}\")\n",
        "    print(f\"Time taken: {time_taken:.2f} seconds\")\n",
        "    print(f\"Peak RAM usage: {peak_mem:.2f} MB\")\n",
        "    print(f\"Ending RAM usage: {end_mem:.2f} MB\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File not found at {compressed_file}. Please check the compression step.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKU951ipm7Tw",
        "outputId": "ea47bc2d-136e-4c21-a535-601319010e7b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Method 3: Pandas (chunksize, gzip)\n",
            "Starting RAM: 587.75 MB\n",
            "\n",
            "--- Results for Method 3 ---\n",
            "Total rows: 184903890\n",
            "Time taken: 224.05 seconds\n",
            "Peak RAM usage: 610.11 MB\n",
            "Ending RAM usage: 610.11 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Comparison and Conclusion\n",
        "\n",
        "Here is a summary of the performance for each method.\n",
        "\n",
        "| Method | Time Taken | Peak RAM Usage\n",
        "| :--- | :---: | :---: |\n",
        "| 1. Pandas (chunksize) | 179.25 sec | 177.50 MB |\n",
        "| 2. Dask | 166.41 sec | 608.21 MB |\n",
        "| 3. Pandas (chunksize, gzip) | 224.05 sec | 610.11 MB |\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "- Dask was the fastest method overall, but it required more RAM.\n",
        "- Pandas with chunksize was slower but very memory efficient.\n",
        "- Pandas with gzip used similar RAM to Dask but was slower due to decompression overhead."
      ],
      "metadata": {
        "id": "lAtudnPIm-N0"
      }
    }
  ]
}